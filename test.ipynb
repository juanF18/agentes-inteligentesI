{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episodios de entrenamiento:  16%|█▌        | 1565/10000 [01:22<07:23, 19.01it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 60\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(max_steps):\n\u001b[1;32m     59\u001b[0m     action \u001b[38;5;241m=\u001b[39m choose_action(state)\n\u001b[0;32m---> 60\u001b[0m     next_state, reward, done, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m discretize_state(next_state, bins)\n\u001b[1;32m     62\u001b[0m     best_next_action \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Q[next_state])\n",
      "File \u001b[0;32m~/Documents/Universidad/2024-1/inteligentes/agentes-inteligentesI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/Documents/Universidad/2024-1/inteligentes/agentes-inteligentesI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Universidad/2024-1/inteligentes/agentes-inteligentesI/.venv/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/Universidad/2024-1/inteligentes/agentes-inteligentesI/.venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/acrobot.py:216\u001b[0m, in \u001b[0;36mAcrobotEnv.step\u001b[0;34m(self, a)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;66;03m# Now, augment the state with our force action so it can be passed to\u001b[39;00m\n\u001b[1;32m    213\u001b[0m \u001b[38;5;66;03m# _dsdt\u001b[39;00m\n\u001b[1;32m    214\u001b[0m s_augmented \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mappend(s, torque)\n\u001b[0;32m--> 216\u001b[0m ns \u001b[38;5;241m=\u001b[39m \u001b[43mrk4\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dsdt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43ms_augmented\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdt\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m ns[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m wrap(ns[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39mpi, pi)\n\u001b[1;32m    219\u001b[0m ns[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m wrap(ns[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39mpi, pi)\n",
      "File \u001b[0;32m~/Documents/Universidad/2024-1/inteligentes/agentes-inteligentesI/.venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/acrobot.py:466\u001b[0m, in \u001b[0;36mrk4\u001b[0;34m(derivs, y0, t)\u001b[0m\n\u001b[1;32m    464\u001b[0m k1 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(derivs(y0))\n\u001b[1;32m    465\u001b[0m k2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(derivs(y0 \u001b[38;5;241m+\u001b[39m dt2 \u001b[38;5;241m*\u001b[39m k1))\n\u001b[0;32m--> 466\u001b[0m k3 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[43mderivs\u001b[49m\u001b[43m(\u001b[49m\u001b[43my0\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mdt2\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    467\u001b[0m k4 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(derivs(y0 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m*\u001b[39m k3))\n\u001b[1;32m    468\u001b[0m yout[i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m y0 \u001b[38;5;241m+\u001b[39m dt \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m6.0\u001b[39m \u001b[38;5;241m*\u001b[39m (k1 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m k2 \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m k3 \u001b[38;5;241m+\u001b[39m k4)\n",
      "File \u001b[0;32m~/Documents/Universidad/2024-1/inteligentes/agentes-inteligentesI/.venv/lib/python3.10/site-packages/gymnasium/envs/classic_control/acrobot.py:259\u001b[0m, in \u001b[0;36mAcrobotEnv._dsdt\u001b[0;34m(self, s_augmented)\u001b[0m\n\u001b[1;32m    255\u001b[0m dtheta1 \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m2\u001b[39m]\n\u001b[1;32m    256\u001b[0m dtheta2 \u001b[38;5;241m=\u001b[39m s[\u001b[38;5;241m3\u001b[39m]\n\u001b[1;32m    257\u001b[0m d1 \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    258\u001b[0m     m1 \u001b[38;5;241m*\u001b[39m lc1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[0;32m--> 259\u001b[0m     \u001b[38;5;241m+\u001b[39m m2 \u001b[38;5;241m*\u001b[39m (l1\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m lc2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m l1 \u001b[38;5;241m*\u001b[39m lc2 \u001b[38;5;241m*\u001b[39m \u001b[43mcos\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtheta2\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    260\u001b[0m     \u001b[38;5;241m+\u001b[39m I1\n\u001b[1;32m    261\u001b[0m     \u001b[38;5;241m+\u001b[39m I2\n\u001b[1;32m    262\u001b[0m )\n\u001b[1;32m    263\u001b[0m d2 \u001b[38;5;241m=\u001b[39m m2 \u001b[38;5;241m*\u001b[39m (lc2\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m l1 \u001b[38;5;241m*\u001b[39m lc2 \u001b[38;5;241m*\u001b[39m cos(theta2)) \u001b[38;5;241m+\u001b[39m I2\n\u001b[1;32m    264\u001b[0m phi2 \u001b[38;5;241m=\u001b[39m m2 \u001b[38;5;241m*\u001b[39m lc2 \u001b[38;5;241m*\u001b[39m g \u001b[38;5;241m*\u001b[39m cos(theta1 \u001b[38;5;241m+\u001b[39m theta2 \u001b[38;5;241m-\u001b[39m pi \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m2.0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import defaultdict\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Crear el entorno\n",
    "env = gym.make(\"Acrobot-v1\")\n",
    "\n",
    "# Parámetros\n",
    "alpha = 0.4  # Tasa de aprendizaje\n",
    "gamma = 0.99  # Factor de descuento\n",
    "epsilon = 0.9  # Parámetro epsilon para la política epsilon-greedy\n",
    "epsilon_min = 0.01  # Mínimo valor de epsilon\n",
    "epsilon_decay = 0.995  # Factor de decaimiento de epsilon\n",
    "num_episodes = 10000  # Número total de episodios de entrenamiento\n",
    "max_steps = 1000  # Número máximo de pasos por episodio\n",
    "num_bins = 10  # Número de bins para discretizar cada dimensión del espacio de estados\n",
    "\n",
    "# Función para discretizar el espacio de estados\n",
    "def discretize_state(state, bins):\n",
    "    state_disc = []\n",
    "    for i in range(len(state)):\n",
    "        state_disc.append(np.digitize(state[i], bins[i]) - 1)\n",
    "    return tuple(state_disc)\n",
    "\n",
    "# Función para crear bins de discretización\n",
    "def create_bins(num_bins, lower_bounds, upper_bounds):\n",
    "    bins = []\n",
    "    for l, u in zip(lower_bounds, upper_bounds):\n",
    "        bins.append(np.linspace(l, u, num_bins))\n",
    "    return bins\n",
    "\n",
    "# Crear bins para la discretización\n",
    "lower_bounds = env.observation_space.low\n",
    "upper_bounds = env.observation_space.high\n",
    "# Ajustar límites superiores para las velocidades (evitar valores infinitos)\n",
    "upper_bounds[1] = 1\n",
    "upper_bounds[3] = 1\n",
    "bins = create_bins(num_bins, lower_bounds, upper_bounds)\n",
    "\n",
    "# Inicializar la Q-Table\n",
    "Q = defaultdict(lambda: np.zeros(env.action_space.n))\n",
    "\n",
    "# Función para elegir acción usando la política epsilon-greedy\n",
    "def choose_action(state):\n",
    "    if np.random.uniform(0, 1) < epsilon:\n",
    "        return env.action_space.sample()\n",
    "    else:\n",
    "        return np.argmax(Q[state])\n",
    "\n",
    "# Entrenamiento del agente usando Q-learning\n",
    "rewards = []\n",
    "for episode in tqdm(range(num_episodes), desc=\"Episodios de entrenamiento\"):\n",
    "    state, _ = env.reset()\n",
    "    state = discretize_state(state, bins)\n",
    "    total_reward = 0\n",
    "    for step in range(max_steps):\n",
    "        action = choose_action(state)\n",
    "        next_state, reward, done, _, _ = env.step(action)\n",
    "        next_state = discretize_state(next_state, bins)\n",
    "        best_next_action = np.argmax(Q[next_state])\n",
    "        Q[state][action] += alpha * (\n",
    "            reward + gamma * Q[next_state][best_next_action] - Q[state][action]\n",
    "        )\n",
    "        state = next_state\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    rewards.append(total_reward)\n",
    "    epsilon = max(epsilon_min, epsilon * epsilon_decay)\n",
    "\n",
    "# Cálculo de media móvil para suavizar las recompensas\n",
    "window_size = 100\n",
    "moving_average_rewards = [np.mean(rewards[i-window_size:i]) for i in range(window_size, len(rewards))]\n",
    "\n",
    "# Graficar las recompensas totales y la media móvil\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(range(num_episodes), rewards, label='Total Rewards per Episode', alpha=0.3, color='blue')\n",
    "plt.plot(range(window_size, num_episodes), moving_average_rewards, label='Moving Average of Total Rewards', color='red')\n",
    "plt.xlabel(\"Episode Number\")\n",
    "plt.ylabel(\"Total Rewards\")\n",
    "plt.title(\"Total Rewards vs Episodes: Acrobot-v1\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
